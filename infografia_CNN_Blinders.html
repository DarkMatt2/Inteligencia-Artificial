<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Infograf√≠a Completa: CNN - Redes Neuronales Convolucionales</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
            line-height: 1.8;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 50px 30px;
            text-align: center;
        }

        header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        header p {
            font-size: 1.2em;
            opacity: 0.95;
        }

        .section {
            padding: 40px 30px;
            border-bottom: 3px solid #f0f0f0;
        }

        .section:last-child {
            border-bottom: none;
        }

        h2 {
            font-size: 2em;
            color: #667eea;
            margin-bottom: 20px;
            border-left: 5px solid #667eea;
            padding-left: 15px;
        }

        h3 {
            font-size: 1.4em;
            color: #764ba2;
            margin-top: 25px;
            margin-bottom: 15px;
        }

        .cnn-explanation {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 25px;
            border-radius: 10px;
            margin-bottom: 20px;
        }

        .cnn-explanation p {
            font-size: 1.05em;
            margin-bottom: 12px;
            text-align: justify;
        }

        .architecture-box {
            background: #f9f9f9;
            border-left: 4px solid #667eea;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .architecture-box strong {
            color: #667eea;
        }

        .component {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 25px 0;
            align-items: center;
        }

        .component-text {
            font-size: 1.05em;
        }

        .component-text h4 {
            color: #764ba2;
            margin-bottom: 10px;
        }

        .dataset-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 25px;
            margin: 30px 0;
        }

        .dataset-card {
            background: linear-gradient(135deg, #ffffff 0%, #f0f0f0 100%);
            border-radius: 10px;
            padding: 25px;
            border: 2px solid #667eea;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .dataset-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(102, 126, 234, 0.3);
        }

        .dataset-header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            border-radius: 5px;
            margin: -25px -25px 15px -25px;
            text-align: center;
        }

        .dataset-header h4 {
            font-size: 1.3em;
            margin-bottom: 5px;
        }

        .dataset-level {
            display: inline-block;
            background: rgba(255, 255, 255, 0.3);
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 0.9em;
            margin-top: 5px;
        }

        .dataset-content p {
            margin: 10px 0;
            font-size: 1em;
        }

        .dataset-label {
            font-weight: bold;
            color: #667eea;
        }

        .metrics-box {
            background: #f0f4ff;
            padding: 20px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #667eea;
        }

        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 15px 0;
        }

        .metric-item {
            background: white;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
            border-top: 3px solid #667eea;
        }

        .metric-label {
            font-size: 0.9em;
            color: #666;
            margin-bottom: 5px;
        }

        .metric-value {
            font-size: 2em;
            font-weight: bold;
            color: #667eea;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            background: white;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
            overflow: hidden;
        }

        .comparison-table thead {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }

        .comparison-table th {
            padding: 18px;
            text-align: left;
            font-weight: 600;
            font-size: 1.05em;
        }

        .comparison-table td {
            padding: 16px 18px;
            border-bottom: 1px solid #e0e0e0;
            font-size: 1em;
        }

        .comparison-table tbody tr {
            transition: background-color 0.3s ease;
        }

        .comparison-table tbody tr:hover {
            background-color: #f5f7fa;
        }

        .comparison-table tbody tr:nth-child(even) {
            background-color: #f9f9f9;
        }

        .status-success {
            background: #d4edda;
            color: #155724;
            padding: 8px 12px;
            border-radius: 5px;
            font-weight: bold;
            text-align: center;
            display: inline-block;
            width: 100%;
            box-sizing: border-box;
        }

        .status-warning {
            background: #fff3cd;
            color: #856404;
            padding: 8px 12px;
            border-radius: 5px;
            font-weight: bold;
            text-align: center;
            display: inline-block;
            width: 100%;
            box-sizing: border-box;
        }

        .precision-high {
            color: #28a745;
            font-weight: bold;
        }

        .precision-medium {
            color: #ffc107;
            font-weight: bold;
        }

        .precision-low {
            color: #dc3545;
            font-weight: bold;
        }

        .code-block {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
            line-height: 1.6;
        }

        .code-block code {
            color: #f8f8f2;
        }

        .keyword { color: #ff79c6; }
        .string { color: #f1fa8c; }
        .function { color: #8be9fd; }
        .number { color: #bd93f9; }

        .highlight-box {
            background: linear-gradient(135deg, #fff5e1 0%, #ffe0b2 100%);
            border-left: 4px solid #ff9800;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
        }

        .highlight-box h4 {
            color: #ff6f00;
            margin-bottom: 10px;
        }

        .success-box {
            background: linear-gradient(135deg, #c8e6c9 0%, #a5d6a7 100%);
            border-left: 4px solid #4caf50;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
        }

        .success-box h4 {
            color: #2e7d32;
            margin-bottom: 10px;
        }

        footer {
            background: #2d3436;
            color: white;
            text-align: center;
            padding: 30px;
            font-size: 0.95em;
        }

        .info-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }

        .info-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
        }

        .info-card h4 {
            font-size: 1.3em;
            margin-bottom: 10px;
        }

        @media (max-width: 768px) {
            .component {
                grid-template-columns: 1fr;
            }

            header h1 {
                font-size: 2em;
            }

            h2 {
                font-size: 1.5em;
            }

            .comparison-table {
                font-size: 0.85em;
            }

            .comparison-table th, .comparison-table td {
                padding: 10px;
            }
        }

        .badge {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 0.85em;
            margin-right: 8px;
            margin-bottom: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- HEADER -->
        <header>
            <h1>üß† Redes Neuronales Convolucionales (CNN)</h1>
            <p>Infograf√≠a Completa: Arquitectura, Datasets, Resultados y An√°lisis Comparativo</p>
        </header>

        <!-- SECCI√ìN 1: ¬øQU√â SON LAS CNN? -->
        <div class="section">
            <h2>üìñ ¬øQu√© son las Redes Neuronales Convolucionales (CNN)?</h2>
            
            <div class="cnn-explanation">
                <p>
                    <strong>Las Redes Neuronales Convolucionales (CNN)</strong> son una arquitectura especializada de Deep Learning dise√±ada para procesar datos que tienen una estructura de cuadr√≠cula, principalmente im√°genes. Utilizan operaciones matem√°ticas llamadas <strong>convoluciones</strong> para extraer caracter√≠sticas visuales de las im√°genes de manera jer√°rquica y autom√°tica.
                </p>
                <p>
                    A diferencia de las redes neuronales tradicionales (MLP - Multilayer Perceptron) que requieren aplanar completamente la imagen, las CNN preservan la estructura espacial de los datos. Esto las hace extraordinariamente eficientes para tareas de <strong>visi√≥n artificial</strong>, como clasificaci√≥n de im√°genes, detecci√≥n de objetos, segmentaci√≥n sem√°ntica y reconocimiento de patrones visuales.
                </p>
                <p>
                    Las CNN han revolucionado campos como conducci√≥n aut√≥noma, medicina (diagn√≥stico por imagen), comercio electr√≥nico (b√∫squeda por imagen), redes sociales y sistemas de vigilancia. Su √©xito radica en la capacidad de aprender autom√°ticamente las caracter√≠sticas m√°s relevantes sin intervenci√≥n manual.
                </p>
            </div>

            <h3>üîß Componentes Principales de una CNN</h3>

            <div class="component">
                <div class="component-text">
                    <h4>1Ô∏è‚É£ Capas de Convoluci√≥n (Conv2D)</h4>
                    <p>
                        La capa convolucional es el coraz√≥n de cualquier CNN. Aplica m√∫ltiples <strong>filtros (kernels)</strong> sobre la imagen de entrada. Cada filtro es una matriz peque√±a (t√≠picamente 3√ó3 o 5√ó5) que se desliza sobre la imagen, realizando operaciones matem√°ticas que detectan caracter√≠sticas como bordes, texturas y patrones m√°s complejos en capas posteriores.
                    </p>
                    <div class="metrics-box">
                        <strong>Par√°metro clave:</strong> N√∫mero de filtros determina cu√°ntas caracter√≠sticas se extraen. Valores t√≠picos: 32, 64, 128.
                    </div>
                </div>
                <div class="architecture-box">
                    <code>
Conv2D(32, (3, 3), activation='relu')<br>
Conv2D(64, (3, 3), activation='relu')<br>
Conv2D(128, (3, 3), activation='relu')
                    </code>
                </div>
            </div>

            <div class="component">
                <div class="component-text">
                    <h4>2Ô∏è‚É£ Capas de Pooling (MaxPooling2D)</h4>
                    <p>
                        El pooling reduce las dimensiones espaciales de los mapas de caracter√≠sticas. <strong>MaxPooling</strong> toma el valor m√°ximo en una regi√≥n peque√±a (t√≠picamente 2√ó2), reduciendo el tama√±o del mapa mientras preserva las caracter√≠sticas m√°s importantes. Esto reduce la cantidad de par√°metros y acelera el entrenamiento.
                    </p>
                    <div class="metrics-box">
                        <strong>Beneficios:</strong> Reduce overfitting, acelera c√°lculos, proporciona invarianza espacial.
                    </div>
                </div>
                <div class="architecture-box">
                    <code>
MaxPooling2D((2, 2))<br>
# Reduce 28√ó28 ‚Üí 14√ó14<br>
# Reduce 14√ó14 ‚Üí 7√ó7
                    </code>
                </div>
            </div>

            <div class="component">
                <div class="component-text">
                    <h4>3Ô∏è‚É£ Batch Normalization</h4>
                    <p>
                        <strong>Batch Normalization</strong> normaliza las entradas de cada capa durante el entrenamiento. Estabiliza el aprendizaje, permite tasas de aprendizaje m√°s altas, y act√∫a como un regularizador d√©bil, reduciendo la dependencia de los pesos inicializados aleatoriamente.
                    </p>
                    <div class="metrics-box">
                        <strong>Ventaja:</strong> Acelera convergencia y mejora generalizaci√≥n significativamente.
                    </div>
                </div>
                <div class="architecture-box">
                    <code>
layers.BatchNormalization()<br>
# Normaliza activaciones<br>
# Acelera entrenamiento
                    </code>
                </div>
            </div>

            <div class="component">
                <div class="component-text">
                    <h4>4Ô∏è‚É£ Dropout (Regularizaci√≥n)</h4>
                    <p>
                        <strong>Dropout</strong> desactiva aleatoriamente una fracci√≥n de neuronas durante el entrenamiento (t√≠picamente 0.2-0.5). Esto previene co-adaptaci√≥n excesiva entre neuronas y es una t√©cnica fundamental para reducir <strong>overfitting</strong>, mejorando la capacidad de generalizaci√≥n del modelo en datos nuevos.
                    </p>
                    <div class="metrics-box">
                        <strong>Regla pr√°ctica:</strong> Aumentar dropout (0.4-0.5) si hay overfitting severo.
                    </div>
                </div>
                <div class="architecture-box">
                    <code>
layers.Dropout(0.25)  # 25% de neuronas<br>
layers.Dropout(0.5)   # 50% de neuronas<br>
# Solo activo durante entrenamiento
                    </code>
                </div>
            </div>

            <div class="component">
                <div class="component-text">
                    <h4>5Ô∏è‚É£ Capas Totalmente Conectadas (Dense) y Softmax</h4>
                    <p>
                        Despu√©s de extraer caracter√≠sticas con convoluciones y pooling, las caracter√≠sticas se aplastan en un vector. Las capas densas (fully connected) toman este vector y realizan clasificaci√≥n final mediante transformaciones lineales seguidas de funciones de activaci√≥n. <strong>Softmax</strong> convierte las salidas en probabilidades normalizadas para cada clase.
                    </p>
                    <div class="metrics-box">
                        <strong>Estructura t√≠pica:</strong> Flatten ‚Üí Dense(256, relu) ‚Üí Dropout ‚Üí Dense(num_classes, softmax)
                    </div>
                </div>
                <div class="architecture-box">
                    <code>
layers.Flatten()<br>
layers.Dense(256, activation='relu')<br>
layers.Dropout(0.5)<br>
layers.Dense(10, activation='softmax')
                    </code>
                </div>
            </div>

            <h3>üéØ Flujo de Datos en una CNN T√≠pica</h3>
            <div class="architecture-box">
                <p>
                    <strong>Imagen de Entrada (28√ó28√ó1)</strong>
                    ‚Üì<br>
                    <strong>Conv2D(32 filtros 3√ó3)</strong> ‚Üí Imagen: 26√ó26√ó32
                    ‚Üì<br>
                    <strong>MaxPooling2D(2√ó2)</strong> ‚Üí Imagen: 13√ó13√ó32
                    ‚Üì<br>
                    <strong>Conv2D(64 filtros 3√ó3)</strong> ‚Üí Imagen: 11√ó11√ó64
                    ‚Üì<br>
                    <strong>MaxPooling2D(2√ó2)</strong> ‚Üí Imagen: 5√ó5√ó64
                    ‚Üì<br>
                    <strong>Flatten</strong> ‚Üí Vector: 1,600 elementos
                    ‚Üì<br>
                    <strong>Dense(128, relu)</strong> ‚Üí Vector: 128 elementos
                    ‚Üì<br>
                    <strong>Dense(10, softmax)</strong> ‚Üí Probabilidades: 10 clases
                </p>
            </div>
        </div>

        <!-- SECCI√ìN 2: DATASETS -->
        <div class="section">
            <h2>üìä Descripci√≥n Detallada de los Datasets</h2>

            <div class="dataset-grid">
                <!-- PRUEBA 1: FASHION-MNIST -->
                <div class="dataset-card">
                    <div class="dataset-header">
                        <h4>üëï Fashion-MNIST</h4>
                        <div class="dataset-level">Nivel: F√°cil</div>
                    </div>
                    <div class="dataset-content">
                        <p><span class="dataset-label">Objetivo:</span> Clasificar 10 categor√≠as de prendas de vestir en im√°genes de escala de grises.</p>
                        
                        <p><span class="dataset-label">üìà Caracter√≠sticas:</span></p>
                        <ul style="margin-left: 15px; margin-bottom: 10px;">
                            <li>70,000 im√°genes totales (60,000 entrenamiento, 10,000 prueba)</li>
                            <li>Tama√±o: 28√ó28 p√≠xeles en escala de grises (1 canal)</li>
                            <li>10 clases: Camiseta, Pantal√≥n, Jersey, Vestido, Abrigo, Sandalia, Camisa, Zapatilla, Bolso, Bot√≠n</li>
                            <li>Datos limpios y bien balanceados</li>
                        </ul>

                        <p><span class="dataset-label">üéØ Aplicaci√≥n Pr√°ctica:</span> Sistemas de recomendaci√≥n de moda, e-commerce, inventarios automatizados.</p>

                        <p><span class="dataset-label">üîç Por qu√© es "F√°cil":</span> Im√°genes simples, fondo uniforme, sin ruido o distorsi√≥n significativa.</p>

                        <div class="metrics-box">
                            <strong>Precisi√≥n Lograda: <span class="precision-high">91.23%</span></strong><br>
                            Super√≥ la meta de 85% establecida.
                        </div>
                    </div>
                </div>

                <!-- PRUEBA 2: CIFAR-10 -->
                <div class="dataset-card">
                    <div class="dataset-header">
                        <h4>üöó CIFAR-10</h4>
                        <div class="dataset-level">Nivel: Intermedio-Bajo</div>
                    </div>
                    <div class="dataset-content">
                        <p><span class="dataset-label">Objetivo:</span> Clasificar objetos cotidianos reales en 10 categor√≠as usando im√°genes a color.</p>
                        
                        <p><span class="dataset-label">üìà Caracter√≠sticas:</span></p>
                        <ul style="margin-left: 15px; margin-bottom: 10px;">
                            <li>60,000 im√°genes totales (50,000 entrenamiento, 10,000 prueba)</li>
                            <li>Tama√±o: 32√ó32 p√≠xeles a color (3 canales RGB)</li>
                            <li>10 clases: Avi√≥n, Autom√≥vil, P√°jaro, Gato, Ciervo, Perro, Rana, Caballo, Barco, Cami√≥n</li>
                            <li>Im√°genes con variabilidad real, orientaciones diferentes, parcialmente ocluidas</li>
                        </ul>

                        <p><span class="dataset-label">üéØ Aplicaci√≥n Pr√°ctica:</span> Sistemas de clasificaci√≥n de objetos, conducci√≥n aut√≥noma, vigilancia inteligente.</p>

                        <p><span class="dataset-label">‚ö†Ô∏è Desaf√≠o:</span> Mayor complejidad visual que Fashion-MNIST; objetos en diferentes √°ngulos, scales y contextos.</p>

                        <div class="metrics-box">
                            <strong>Precisi√≥n Lograda: <span class="precision-medium">74.50%</span></strong><br>
                            ‚ö†Ô∏è Qued√≥ 0.50% por debajo de la meta de 75%.
                        </div>
                    </div>
                </div>

                <!-- PRUEBA 3: SVHN -->
                <div class="dataset-card">
                    <div class="dataset-header">
                        <h4>üèòÔ∏è SVHN (Street View House Numbers)</h4>
                        <div class="dataset-level">Nivel: Intermedio</div>
                    </div>
                    <div class="dataset-content">
                        <p><span class="dataset-label">Objetivo:</span> Clasificar d√≠gitos (0-9) en im√°genes reales de n√∫meros de viviendas tomadas de Google Street View.</p>
                        
                        <p><span class="dataset-label">üìà Caracter√≠sticas:</span></p>
                        <ul style="margin-left: 15px; margin-bottom: 10px;">
                            <li>~600,000 im√°genes totales (cropped a 32√ó32 p√≠xeles)</li>
                            <li>Formato: im√°genes RGB a color de n√∫meros reales en fotograf√≠as urbanas</li>
                            <li>10 clases: D√≠gitos 0-9</li>
                            <li>Mayor variabilidad: diferentes iluminaciones, √°ngulos, fondos complejos, parcialmente borrados</li>
                        </ul>

                        <p><span class="dataset-label">üéØ Aplicaci√≥n Pr√°ctica:</span> OCR (Reconocimiento √ìptico de Caracteres), lectura autom√°tica de direcciones, mapas inteligentes, sistemas de navegaci√≥n.</p>

                        <p><span class="dataset-label">üåç Puente Importante:</span> Representa un punto medio entre datos sint√©ticos (MNIST) y completamente del mundo real, demostrando transici√≥n a problemas pr√°cticos.</p>

                        <div class="metrics-box">
                            <strong>Precisi√≥n Lograda: <span class="precision-high">93.82%</span></strong><br>
                            Super√≥ ampliamente la meta de 85%.
                        </div>
                    </div>
                </div>

                <!-- PRUEBA 4: EMNIST -->
                <div class="dataset-card">
                    <div class="dataset-header">
                        <h4>‚úçÔ∏è EMNIST Balanced</h4>
                        <div class="dataset-level">Nivel: Intermedio-Alto</div>
                    </div>
                    <div class="dataset-content">
                        <p><span class="dataset-label">Objetivo:</span> Clasificar caracteres manuscritos alfanum√©ricos (letras may√∫sculas, min√∫sculas y d√≠gitos) en 47 clases.</p>
                        
                        <p><span class="dataset-label">üìà Caracter√≠sticas:</span></p>
                        <ul style="margin-left: 15px; margin-bottom: 10px;">
                            <li>~750,000 im√°genes totales (versi√≥n "Balanced" con 47 clases)</li>
                            <li>Tama√±o: 28√ó28 p√≠xeles en escala de grises</li>
                            <li>47 clases: D√≠gitos (0-9) + Letras may√∫sculas (A-Z) + Letras min√∫sculas (a-z)</li>
                            <li>Mayor complejidad por n√∫mero de clases y similitud entre caracteres</li>
                        </ul>

                        <p><span class="dataset-label">üéØ Aplicaci√≥n Pr√°ctica:</span> OCR manuscrito, digitalizaci√≥n de documentos, procesamiento de formularios escritos a mano, accesibilidad.</p>

                        <p><span class="dataset-label">üé≤ Desaf√≠o Multiclase:</span> 47 clases vs 10 en MNIST; similitud visual entre "l" y "1", "O" y "0", etc.</p>

                        <div class="metrics-box">
                            <strong>Precisi√≥n Lograda: <span class="precision-high">87.89%</span></strong><br>
                            Super√≥ la meta de 80%.
                        </div>
                    </div>
                </div>

                <!-- PRUEBA 5: TINY IMAGENET -->
                <div class="dataset-card">
                    <div class="dataset-header">
                        <h4>üñºÔ∏è Tiny ImageNet</h4>
                        <div class="dataset-level">Nivel: Avanzado</div>
                    </div>
                    <div class="dataset-content">
                        <p><span class="dataset-label">Objetivo:</span> Clasificar im√°genes en 200 clases usando Transfer Learning con MobileNetV2 preentrenado.</p>
                        
                        <p><span class="dataset-label">üìà Caracter√≠sticas:</span></p>
                        <ul style="margin-left: 15px; margin-bottom: 10px;">
                            <li>~100,000 im√°genes totales (64,000 entrenamiento, 10,000 validaci√≥n, 10,000 prueba)</li>
                            <li>Tama√±o: 64√ó64 p√≠xeles a color (ampliadas a 96√ó96 para MobileNetV2)</li>
                            <li>200 clases: Objetos diversos (animales, instrumentos, veh√≠culos, etc.)</li>
                            <li>Benchmark intermedio entre CIFAR-10 y ImageNet completo (1000 clases)</li>
                        </ul>

                        <p><span class="dataset-label">üéØ Aplicaci√≥n Pr√°ctica:</span> Clasificaci√≥n a gran escala, b√∫squeda de im√°genes, visi√≥n rob√≥tica, reconocimiento de objetos en conducci√≥n aut√≥noma.</p>

                        <p><span class="dataset-label">üöÄ T√©cnica Especial:</span> Uso de Transfer Learning + Fine-tuning para aprovechar conocimiento preentrenado en ImageNet.</p>

                        <div class="metrics-box">
                            <strong>Precisi√≥n Lograda: <span class="precision-medium">59.35%</span></strong><br>
                            ‚ö†Ô∏è Qued√≥ 0.65% por debajo de la meta de 60%. 200 clases representan mayor complejidad.
                        </div>
                    </div>
                </div>

                <!-- PRUEBA 6: PLANTVILLAGE -->
                <div class="dataset-card">
                    <div class="dataset-header">
                        <h4>üå± PlantVillage</h4>
                        <div class="dataset-level">Nivel: Aplicado</div>
                    </div>
                    <div class="dataset-content">
                        <p><span class="dataset-label">Objetivo:</span> Detectar enfermedades en plantas mediante clasificaci√≥n multiclase de im√°genes de hojas.</p>
                        
                        <p><span class="dataset-label">üìà Caracter√≠sticas:</span></p>
                        <ul style="margin-left: 15px; margin-bottom: 10px;">
                            <li>54,303 im√°genes totales de hojas de plantas fotografiadas en laboratorio</li>
                            <li>Tama√±o: Variable (~256√ó256) redimensionadas a 224√ó224 para MobileNetV2</li>
                            <li>38 clases: Combinaci√≥n de 14 especies √ó estado de salud/enfermedad</li>
                            <li>Im√°genes reales con variabilidad de iluminaci√≥n, √°ngulos, fondos</li>
                        </ul>

                        <p><span class="dataset-label">üéØ Aplicaci√≥n Pr√°ctica:</span> Diagn√≥stico agr√≠cola automatizado, detecci√≥n temprana de enfermedades, agricultura sostenible, soporte a agricultores en regiones en desarrollo.</p>

                        <p><span class="dataset-label">üíö Impacto Social:</span> Problema real y socialmente relevante que beneficia a la agricultura y seguridad alimentaria.</p>

                        <div class="metrics-box">
                            <strong>Precisi√≥n Lograda: <span class="precision-high">86.03%</span></strong><br>
                            Super√≥ la meta de 85%.
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- SECCI√ìN 3: ARQUITECTURA DETALLADA POR PRUEBA -->
        <div class="section">
            <h2>‚öôÔ∏è Arquitectura y C√≥digo Python de Cada Prueba</h2>

            <h3>Prueba 1 & 2: Fashion-MNIST y CIFAR-10 (Arquitecturas CNN B√°sicas)</h3>
            <div class="code-block">
<span class="keyword">model</span> = models.Sequential([
    layers.Input(shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),
    
    <span class="comment"># Bloque 1: Convoluci√≥n + Pooling</span>
    layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>),
    layers.BatchNormalization(),
    layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)),
    layers.Dropout(<span class="number">0.25</span>),
    
    <span class="comment"># Bloque 2: Convoluci√≥n + Pooling</span>
    layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>),
    layers.BatchNormalization(),
    layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)),
    layers.Dropout(<span class="number">0.3</span>),
    
    <span class="comment"># Capas Densas</span>
    layers.Flatten(),
    layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),
    layers.Dropout(<span class="number">0.3</span>),
    layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>)
])

model.compile(
    optimizer=<span class="string">'adam'</span>,
    loss=<span class="string">'sparse_categorical_crossentropy'</span>,
    metrics=[<span class="string">'accuracy'</span>]
)
            </div>

            <h3>Prueba 3 & 4: SVHN y EMNIST (Arquitecturas CNN Profundas)</h3>
            <div class="code-block">
<span class="keyword">model</span> = models.Sequential([
    layers.Input(shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)),
    
    <span class="comment"># Bloque 1</span>
    layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>),
    layers.BatchNormalization(),
    layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>),
    layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)),
    layers.Dropout(<span class="number">0.25</span>),
    
    <span class="comment"># Bloque 2</span>
    layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>),
    layers.BatchNormalization(),
    layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>),
    layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)),
    layers.Dropout(<span class="number">0.25</span>),
    
    <span class="comment"># Bloque 3 (SVHN/EMNIST es m√°s profundo)</span>
    layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>),
    layers.BatchNormalization(),
    layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)),
    layers.Dropout(<span class="number">0.4</span>),
    
    layers.Flatten(),
    layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>),
    layers.Dropout(<span class="number">0.5</span>),
    layers.Dense(<span class="number">47</span>, activation=<span class="string">'softmax'</span>)  <span class="comment"># 47 para EMNIST, 10 para SVHN</span>
])
            </div>

            <h3>Prueba 5 & 6: Transfer Learning (Tiny ImageNet y PlantVillage)</h3>
            <div class="code-block">
<span class="comment"># Cargar modelo preentrenado</span>
base_model = applications.MobileNetV2(
    input_shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>),
    include_top=<span class="keyword">False</span>,
    weights=<span class="string">'imagenet'</span>
)

<span class="comment"># Congelar capas base inicialmente</span>
base_model.trainable = <span class="keyword">False</span>

<span class="comment"># Agregar cabeza personalizada</span>
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.BatchNormalization(),
    layers.Dense(<span class="number">1024</span>, activation=<span class="string">'relu'</span>),
    layers.Dropout(<span class="number">0.5</span>),
    layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>),
    layers.Dropout(<span class="number">0.3</span>),
    layers.Dense(<span class="number">200</span>, activation=<span class="string">'softmax'</span>)  <span class="comment"># 200 para Tiny ImageNet</span>
])

<span class="comment"># FASE 1: Entrenamiento con base congelada</span>
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>),
    loss=<span class="string">'categorical_crossentropy'</span>,
    metrics=[<span class="string">'accuracy'</span>]
)
history1 = model.fit(train_data, epochs=<span class="number">25</span>, validation_data=val_data)

<span class="comment"># FASE 2: Fine-tuning - Descongelar √∫ltimas capas</span>
base_model.trainable = <span class="keyword">True</span>
<span class="keyword">for</span> layer <span class="keyword">in</span> base_model.layers[:-<span class="number">30</span>]:
    layer.trainable = <span class="keyword">False</span>

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=<span class="number">0.0001</span>),
    loss=<span class="string">'categorical_crossentropy'</span>,
    metrics=[<span class="string">'accuracy'</span>]
)
history2 = model.fit(train_data, epochs=<span class="number">15</span>, validation_data=val_data)
            </div>

            <h3>T√©cnicas Utilizadas en Todas las Pruebas</h3>
            <div class="info-grid">
                <div class="info-card">
                    <h4>üìä Normalizaci√≥n</h4>
                    <p>Dividir por 255.0 para llevar valores a rango [0, 1], mejorando estabilidad del entrenamiento.</p>
                </div>
                <div class="info-card">
                    <h4>üîÑ Data Augmentation</h4>
                    <p>Rotaciones, zoom, flip horizontal/vertical para aumentar variedad de datos y reducir overfitting.</p>
                </div>
                <div class="info-card">
                    <h4>‚è∏Ô∏è Callbacks</h4>
                    <p>ReduceLROnPlateau, EarlyStopping para optimizar entrenamiento din√°micamente.</p>
                </div>
                <div class="info-card">
                    <h4>üîß Optimizador Adam</h4>
                    <p>Combina ventajas de momentum y RMSprop, muy efectivo para redes profundas.</p>
                </div>
            </div>
        </div>

        <!-- SECCI√ìN 4: RESULTADOS DETALLADOS -->
        <div class="section">
            <h2>üìà Resultados y M√©tricas por Prueba</h2>

            <h3>Prueba 1: Fashion-MNIST</h3>
            <div class="metrics-grid">
                <div class="metric-item">
                    <div class="metric-label">Precisi√≥n Final</div>
                    <div class="metric-value precision-high">91.23%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Meta Establecida</div>
                    <div class="metric-value" style="color: #ff9800;">‚â• 85%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Estado</div>
                    <div class="metric-value precision-high">‚úì CUMPLE</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">√âpocas</div>
                    <div class="metric-value">10</div>
                </div>
            </div>
            <div class="success-box">
                <h4>‚úÖ An√°lisis: Fashion-MNIST</h4>
                <p>El modelo alcanz√≥ <strong>91.23%</strong> de precisi√≥n, superando significativamente la meta de 85%. Este excelente rendimiento es esperado debido a la simplicidad del dataset: im√°genes limpias, bien balanceadas y fondo uniforme. El modelo aprendi√≥ eficientemente las caracter√≠sticas distintivas de cada prenda de vestir.</p>
            </div>

            <h3>Prueba 2: CIFAR-10</h3>
            <div class="metrics-grid">
                <div class="metric-item">
                    <div class="metric-label">Precisi√≥n Final</div>
                    <div class="metric-value precision-medium">74.50%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Meta Establecida</div>
                    <div class="metric-value" style="color: #ff9800;">‚â• 75%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Diferencia</div>
                    <div class="metric-value precision-low">-0.50%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">√âpocas</div>
                    <div class="metric-value">10</div>
                </div>
            </div>
            <div class="highlight-box">
                <h4>‚ö†Ô∏è An√°lisis: CIFAR-10</h4>
                <p>El modelo alcanz√≥ <strong>74.50%</strong>, quedando marginalmente (0.50%) por debajo de la meta de 75%. Este resultado refleja la mayor complejidad de CIFAR-10 respecto a Fashion-MNIST: objetos en diferentes √°ngulos, scales variados, fondos complejos y oclusiones parciales. Con ajustes finos en hiperpar√°metros o arquitectura m√°s profunda podr√≠a alcanzarse la meta.</p>
                <p><strong>Posibles mejoras:</strong> Aumentar √©pocas, usar Data Augmentation m√°s agresivo, o arquitectura m√°s profunda (ResNet).</p>
            </div>

            <h3>Prueba 3: SVHN</h3>
            <div class="metrics-grid">
                <div class="metric-item">
                    <div class="metric-label">Precisi√≥n Final</div>
                    <div class="metric-value precision-high">93.82%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Meta Establecida</div>
                    <div class="metric-value" style="color: #ff9800;">‚â• 85%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Super√≥ por</div>
                    <div class="metric-value precision-high">+8.82%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">√âpocas</div>
                    <div class="metric-value">10</div>
                </div>
            </div>
            <div class="success-box">
                <h4>‚úÖ An√°lisis: SVHN</h4>
                <p>El modelo alcanz√≥ <strong>93.82%</strong>, muy por encima de la meta de 85%. A pesar de la variabilidad real en datos (iluminaci√≥n diferente, √°ngulos, fondos complejos), la CNN de 3 capas logr√≥ aprender caracter√≠sticas robustas. Este resultado demuestra la efectividad de Batch Normalization, Dropout y arquitecturas profundas para datos del mundo real.</p>
            </div>

            <h3>Prueba 4: EMNIST Balanced</h3>
            <div class="metrics-grid">
                <div class="metric-item">
                    <div class="metric-label">Precisi√≥n Final</div>
                    <div class="metric-value precision-high">87.89%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Meta Establecida</div>
                    <div class="metric-value" style="color: #ff9800;">‚â• 80%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Super√≥ por</div>
                    <div class="metric-value precision-high">+7.89%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Clases</div>
                    <div class="metric-value">47</div>
                </div>
            </div>
            <div class="success-box">
                <h4>‚úÖ An√°lisis: EMNIST Balanced</h4>
                <p>El modelo alcanz√≥ <strong>87.89%</strong> con 47 clases, superando la meta de 80%. La complejidad aument√≥ significativamente (47 clases vs 10 en MNIST), pero el modelo mantuvo excelente rendimiento. La arquitectura m√°s profunda (3 bloques de convoluci√≥n) y mayor regularizaci√≥n (dropout hasta 0.5) fueron clave. La similitud entre caracteres (0/O, 1/l) genera confusiones residuales.</p>
            </div>

            <h3>Prueba 5: Tiny ImageNet</h3>
            <div class="metrics-grid">
                <div class="metric-item">
                    <div class="metric-label">Precisi√≥n Final</div>
                    <div class="metric-value precision-medium">59.35%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Meta Establecida</div>
                    <div class="metric-value" style="color: #ff9800;">‚â• 60%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Diferencia</div>
                    <div class="metric-value precision-low">-0.65%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Clases</div>
                    <div class="metric-value">200</div>
                </div>
            </div>
            <div class="highlight-box">
                <h4>‚ö†Ô∏è An√°lisis: Tiny ImageNet</h4>
                <p>El modelo alcanz√≥ <strong>59.35%</strong>, quedando 0.65% por debajo de la meta de 60%. Con 200 clases (vs 10-47 en pruebas anteriores), el desaf√≠o es exponencialmente mayor. Transfer Learning con MobileNetV2 fue crucial para evitar peor rendimiento. El fine-tuning selectivo en √∫ltimas capas mejor√≥ resultados vs mantener base completamente congelada.</p>
                <p><strong>Contexto:</strong> 60% en 200 clases es comparable a 90%+ en tareas de 10 clases. Mejoras requieren arquitecturas m√°s profundas (ResNet, EfficientNet) o datasets adicionales.</p>
            </div>

            <h3>Prueba 6: PlantVillage</h3>
            <div class="metrics-grid">
                <div class="metric-item">
                    <div class="metric-label">Precisi√≥n Final</div>
                    <div class="metric-value precision-high">86.03%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Meta Establecida</div>
                    <div class="metric-value" style="color: #ff9800;">‚â• 85%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Super√≥ por</div>
                    <div class="metric-value precision-high">+1.03%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Aplicaci√≥n</div>
                    <div class="metric-value">Agr√≠cola</div>
                </div>
            </div>
            <div class="success-box">
                <h4>‚úÖ An√°lisis: PlantVillage</h4>
                <p>El modelo alcanz√≥ <strong>86.03%</strong>, superando la meta de 85%. A pesar del desbalanceo de clases y variabilidad en im√°genes reales de hojas, Transfer Learning con MobileNetV2 permiti√≥ aprender caracter√≠sticas discriminativas. Data Augmentation intenso fue esencial. Este resultado demuestra viabilidad de sistemas de diagn√≥stico agr√≠cola automatizado.</p>
            </div>

            <h3>Ejercicio Interesante: Evaluaci√≥n Comparativa MNIST vs SVHN</h3>
            <div class="metrics-grid">
                <div class="metric-item">
                    <div class="metric-label">Precisi√≥n MNIST</div>
                    <div class="metric-value precision-high">98.85%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Meta MNIST</div>
                    <div class="metric-value" style="color: #ff9800;">‚â• 95%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Precisi√≥n SVHN</div>
                    <div class="metric-value precision-high">84.94%</div>
                </div>
                <div class="metric-item">
                    <div class="metric-label">Meta SVHN</div>
                    <div class="metric-value" style="color: #ff9800;">‚â• 80%</div>
                </div>
            </div>
            <div class="success-box">
                <h4>‚úÖ An√°lisis: MNIST vs SVHN</h4>
                <p>Este ejercicio comparativo es extremadamente revelador. Utilizando <strong>la misma arquitectura CNN</strong>, el modelo logr√≥ <strong>98.85%</strong> en MNIST vs <strong>84.94%</strong> en SVHN: una diferencia de casi <strong>14 puntos porcentuales</strong>.</p>
                <p><strong>Interpretaci√≥n:</strong> MNIST tiene d√≠gitos manuscritos simples y limpios (fondo blanco, escritura clara). SVHN tiene n√∫meros reales en fotograf√≠as urbanas (iluminaci√≥n variable, fondos complejos, √°ngulos diversos). Esta brecha ilustra por qu√© problemas reales son m√°s desafiantes y por qu√© Transfer Learning es valioso.</p>
            </div>
        </div>

        <!-- SECCI√ìN 5: TABLA COMPARATIVA COMPLETA -->
        <div class="section">
            <h2>üìä Cuadro Comparativo Completo de Todas las Pruebas</h2>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Prueba</th>
                        <th>Nivel</th>
                        <th>Dataset</th>
                        <th>Clases</th>
                        <th>Tama√±o (px)</th>
                        <th>Canales</th>
                        <th>Im√°genes Totales</th>
                        <th>T√©cnicas Clave</th>
                        <th>Precisi√≥n (%)</th>
                        <th>Meta (%)</th>
                        <th>Estado</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>1. Fashion-MNIST</strong></td>
                        <td>F√°cil</td>
                        <td>Prendas de vestir</td>
                        <td>10</td>
                        <td>28√ó28</td>
                        <td>1 (Gray)</td>
                        <td>70,000</td>
                        <td>CNN b√°sica, Dropout</td>
                        <td><span class="precision-high">91.23</span></td>
                        <td>‚â• 85</td>
                        <td><span class="status-success">‚úì Cumple</span></td>
                    </tr>
                    <tr>
                        <td><strong>2. CIFAR-10</strong></td>
                        <td>Intermedio-Bajo</td>
                        <td>Objetos reales</td>
                        <td>10</td>
                        <td>32√ó32</td>
                        <td>3 (RGB)</td>
                        <td>60,000</td>
                        <td>Batch Norm, Dropout, Padding</td>
                        <td><span class="precision-medium">74.50</span></td>
                        <td>‚â• 75</td>
                        <td><span class="status-warning">‚ö† Margen: -0.50%</span></td>
                    </tr>
                    <tr>
                        <td><strong>3. SVHN</strong></td>
                        <td>Intermedio</td>
                        <td>N√∫meros urbanos reales</td>
                        <td>10</td>
                        <td>32√ó32</td>
                        <td>3 (RGB)</td>
                        <td>~600,000</td>
                        <td>CNN profunda, Batch Norm, Prefetch</td>
                        <td><span class="precision-high">93.82</span></td>
                        <td>‚â• 85</td>
                        <td><span class="status-success">‚úì Cumple</span></td>
                    </tr>
                    <tr>
                        <td><strong>4. EMNIST</strong></td>
                        <td>Intermedio-Alto</td>
                        <td>Caracteres manuscritos</td>
                        <td>47</td>
                        <td>28√ó28</td>
                        <td>1 (Gray)</td>
                        <td>~750,000</td>
                        <td>CNN profunda, 3 bloques, Regularizaci√≥n alta</td>
                        <td><span class="precision-high">87.89</span></td>
                        <td>‚â• 80</td>
                        <td><span class="status-success">‚úì Cumple</span></td>
                    </tr>
                    <tr>
                        <td><strong>5. Tiny ImageNet</strong></td>
                        <td>Avanzado</td>
                        <td>Objetos diversos</td>
                        <td>200</td>
                        <td>96√ó96</td>
                        <td>3 (RGB)</td>
                        <td>~100,000</td>
                        <td>Transfer Learning, MobileNetV2, Fine-tuning</td>
                        <td><span class="precision-medium">59.35</span></td>
                        <td>‚â• 60</td>
                        <td><span class="status-warning">‚ö† Margen: -0.65%</span></td>
                    </tr>
                    <tr>
                        <td><strong>6. PlantVillage</strong></td>
                        <td>Aplicado</td>
                        <td>Enfermedades en plantas</td>
                        <td>38</td>
                        <td>224√ó224</td>
                        <td>3 (RGB)</td>
                        <td>54,303</td>
                        <td>Transfer Learning, Data Augmentation intenso</td>
                        <td><span class="precision-high">86.03</span></td>
                        <td>‚â• 85</td>
                        <td><span class="status-success">‚úì Cumple</span></td>
                    </tr>
                    <tr style="background: #f0f4ff; font-weight: bold;">
                        <td colspan="8">EJERCICIO INTERESANTE: Evaluaci√≥n Comparativa MNIST vs SVHN</td>
                        <td colspan="3"></td>
                    </tr>
                    <tr>
                        <td><strong>MNIST</strong></td>
                        <td>Comparativo</td>
                        <td>D√≠gitos manuscritos limpios</td>
                        <td>10</td>
                        <td>32√ó32</td>
                        <td>1 (Gray)</td>
                        <td>70,000</td>
                        <td>CNN est√°ndar, misma arquitectura</td>
                        <td><span class="precision-high">98.85</span></td>
                        <td>‚â• 95</td>
                        <td><span class="status-success">‚úì Cumple</span></td>
                    </tr>
                    <tr>
                        <td><strong>SVHN</strong></td>
                        <td>Comparativo</td>
                        <td>N√∫meros en contexto real urbano</td>
                        <td>10</td>
                        <td>32√ó32</td>
                        <td>3 (RGB)</td>
                        <td>~600,000</td>
                        <td>CNN est√°ndar, misma arquitectura</td>
                        <td><span class="precision-high">84.94</span></td>
                        <td>‚â• 80</td>
                        <td><span class="status-success">‚úì Cumple</span></td>
                    </tr>
                </tbody>
            </table>

            <div class="highlight-box" style="margin-top: 30px;">
                <h4>üìå Conclusiones del Cuadro Comparativo</h4>
                <ul style="margin-left: 20px;">
                    <li><strong>4 de 6 pruebas cumplieron metas:</strong> Fashion-MNIST, SVHN, EMNIST y PlantVillage (66.7% tasa de √©xito).</li>
                    <li><strong>2 pruebas quedaron marginalmente bajo objetivo:</strong> CIFAR-10 (-0.50%) y Tiny ImageNet (-0.65%), indicando que con optimizaciones menores podr√≠an cumplirse.</li>
                    <li><strong>Relaci√≥n inversa: complejidad vs precisi√≥n:</strong> A mayor complejidad (m√°s clases, datos ruidosos), menor precisi√≥n absoluta, pero rendimiento relativo se mantiene s√≥lido.</li>
                    <li><strong>Transfer Learning es clave para muchas clases:</strong> Tiny ImageNet (200 clases) y PlantVillage (38 clases) demostraron la importancia de leveraging conocimiento preentrenado.</li>
                    <li><strong>Diferencia MNIST vs SVHN:</strong> 14 puntos porcentuales ilustran impacto de calidad y complejidad de datos en rendimiento final.</li>
                </ul>
            </div>
        </div>

        <!-- SECCI√ìN 6: INSIGHTS Y RECOMENDACIONES -->
        <div class="section">
            <h2>üí° Insights Clave y Recomendaciones</h2>

            <h3>üîç Hallazgos Principales</h3>
            <div class="info-grid">
                <div class="info-card">
                    <h4>1. Complejidad de Datos</h4>
                    <p>La precisi√≥n depende m√°s de la complejidad visual del dataset que del tama√±o. SVHN (600K im√°genes) logr√≥ 93.82%, mientras que CIFAR-10 (60K im√°genes) logr√≥ 74.50%.</p>
                </div>
                <div class="info-card">
                    <h4>2. Escalabilidad de Clases</h4>
                    <p>Con 200 clases (Tiny ImageNet), la precisi√≥n disminuye relativamente menos de lo esperado (59.35%) con Transfer Learning, vs CNN desde cero.</p>
                </div>
                <div class="info-card">
                    <h4>3. Regularizaci√≥n Esencial</h4>
                    <p>Batch Normalization + Dropout fue cr√≠tico en todos los casos. Sin estos, overfitting habr√≠a sido severo en datasets peque√±os.</p>
                </div>
                <div class="info-card">
                    <h4>4. Aplicaciones Reales Viables</h4>
                    <p>86% en PlantVillage demuestra que sistemas de diagn√≥stico autom√°tico agr√≠cola son factibles y pr√°cticos.</p>
                </div>
            </div>

            <h3>üöÄ Recomendaciones para Mejora</h3>
            <div class="architecture-box">
                <p><strong>Para CIFAR-10 (Meta no alcanzada):</strong></p>
                <ul style="margin-left: 15px; margin-top: 10px;">
                    <li>Aumentar √©pocas de 10 a 20-30 para mayor convergencia</li>
                    <li>Implementar arquitectura ResNet o DenseNet m√°s profunda</li>
                    <li>Aplicar Data Augmentation m√°s agresivo (rotaci√≥n +25¬∞, zoom +25%)</li>
                    <li>Reducir Learning Rate inicial de 0.001 a 0.0005</li>
                    <li>Usar ensemble de m√∫ltiples modelos para votaci√≥n</li>
                </ul>
            </div>

            <div class="architecture-box">
                <p><strong>Para Tiny ImageNet (Meta no alcanzada):</strong></p>
                <ul style="margin-left: 15px; margin-top: 10px;">
                    <li>Explorar arquitecturas m√°s potentes: ResNet152, EfficientNetB4</li>
                    <li>Aumentar √©pocas de fine-tuning de 15 a 25-30</li>
                    <li>Usar mixup o cutmix Data Augmentation</li>
                    <li>Implementar Progressive Resizing (entrenar a resolutions crecientes)</li>
                    <li>Usar model distillation o knowledge transfer de modelos mayores</li>
                </ul>
            </div>

            <div class="success-box">
                <h4>‚ú® Mejores Pr√°cticas Identificadas</h4>
                <ul style="margin-left: 15px;">
                    <li>Siempre normalizar datos (dividir por 255 para im√°genes)</li>
                    <li>Usar Adam optimizer como default para redes profundas</li>
                    <li>Implementar Batch Normalization despu√©s de convoluciones</li>
                    <li>Aplicar Dropout progresivo (aumentar en capas posteriores)</li>
                    <li>Para datasets peque√±os (<100K), usar Transfer Learning</li>
                    <li>Visualizar algunos resultados (confusion matrix) para diagn√≥stico</li>
                    <li>Guardar siempre el mejor modelo durante entrenamiento</li>
                </ul>
            </div>
        </div>

        <!-- FOOTER -->
        <footer>
            <p><strong>Infograf√≠a Generada:</strong> Taller 2.5 - Redes Neuronales Convolucionales (CNN)</p>
            <p><strong>Universidad:</strong> Universidad Andina del Cusco | <strong>Carrera:</strong> Ingenier√≠a de Sistemas</p>
            <p><strong>Asignatura:</strong> Inteligencia Artificial (10A) | <strong>Docente:</strong> Mg. Ing. Hugo Espetia Huamanga</p>
            <p><strong>Equipo de Desarrollo:</strong> Blinders | <strong>A√±o Acad√©mico:</strong> 2025-II</p>
            <p style="margin-top: 15px; font-size: 0.9em; opacity: 0.8;">¬© 2025 - Todos los derechos reservados. Esta infograf√≠a contiene informaci√≥n educativa sobre CNNs.</p>
        </footer>
    </div>
</body>
</html>
